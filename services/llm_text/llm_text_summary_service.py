import asyncio
from services.ollama_client import OllamaClient
from models import TextSummary
from services.llm_text.prompt_builder import SummaryPromptBuilder


class LLMTextSummaryService:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–≤—É—è–∑—ã—á–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ —Ç–µ–∫—Å—Ç–∞ (RU + EN) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama."""

    def __init__(self, client: OllamaClient):
        self.client = client

    async def generate(
        self,
        text: str,
        sentences: int = 8,
        max_attempts: int = 3
    ) -> TextSummary:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö."""
        prompt = SummaryPromptBuilder.build_dual_lang_prompt(text, sentences=sentences)

        for attempt in range(1, max_attempts + 1):
            print(f"\nüîÅ –ü–æ–ø—ã—Ç–∫–∞ {attempt} (summary)...")
            resp = await self.client.async_ask(prompt, schema=TextSummary)

            if isinstance(resp, TextSummary):
                print("‚úÖ –ü–æ–ª—É—á–µ–Ω –≤–∞–ª–∏–¥–Ω—ã–π –æ–±—ä–µ–∫—Ç TextSummary.")
                return resp

            print(f"‚ùå –û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å TextSummary. –û—Ç–≤–µ—Ç: {resp}")
            await asyncio.sleep(1.0)

        raise ValueError("‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã ‚Äî –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—é–º–µ.")
